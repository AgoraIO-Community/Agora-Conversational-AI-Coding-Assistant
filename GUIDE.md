# Building a Voice-Powered AI Coding Assistant with Agora Conversational AI

When I first saw the possibilities of voice-driven development tools, I knew we had to build something that would blow developers' minds at LA Tech Week. Not just another chatbot, but a real-time coding assistant that listens to your voice and generates working web applications instantly.

This guide walks you through how we built it using Agora's Conversational AI platform. You'll learn the architecture decisions, the tricky parts we solved, and how to build your own voice-powered coding assistant.

## What We're Building

An AI coding assistant that:

- Listens to your voice using Agora RTC (Real-Time Communication)
- Processes your requests through GPT-4o
- Responds with natural speech via Azure TTS
- Generates HTML/CSS/JavaScript code that renders live in your browser
- Keeps preview and code visible even after ending the session

Watch it in action: Ask "Create a todo list app with a purple gradient" and within seconds, you'll see a fully functional app render in the preview pane while the AI explains what it built.

## Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        User's Browser                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Voice Input    ‚îÇ  Next.js App    ‚îÇ    Code Preview        ‚îÇ
‚îÇ   (Microphone)   ‚îÇ  (React + TS)   ‚îÇ   (Sandboxed iframe)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ                    ‚îÇ
         ‚îÇ                  ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Agora   ‚îÇ        ‚îÇ Next.js ‚îÇ         ‚îÇ Client  ‚îÇ
    ‚îÇ   RTC   ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   API   ‚îÇ         ‚îÇ  State  ‚îÇ
    ‚îÇ (Audio) ‚îÇ        ‚îÇ Routes  ‚îÇ         ‚îÇManagement‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                  ‚îÇ
         ‚îÇ             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ             ‚îÇ  Agora Conversational AI    ‚îÇ
         ‚îÇ             ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚îÇ  ‚Ä¢ ASR (Speech‚ÜíText) ‚îÇ   ‚îÇ
                       ‚îÇ  ‚îÇ  ‚Ä¢ LLM (GPT-4o)      ‚îÇ   ‚îÇ
                       ‚îÇ  ‚îÇ  ‚Ä¢ TTS (Text‚ÜíSpeech) ‚îÇ   ‚îÇ
                       ‚îÇ  ‚îÇ  ‚Ä¢ RTM (Transcripts) ‚îÇ   ‚îÇ
                       ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

1. **Frontend (Next.js + React)**: Handles UI, state management, and real-time preview
2. **Agora RTC SDK**: Bidirectional audio streaming
3. **Agora RTM SDK**: Real-time messaging for transcripts
4. **Agora Conversational AI**: Orchestrates ASR ‚Üí LLM ‚Üí TTS pipeline
5. **API Routes**: Server-side token generation and agent management

## The Flow: From Voice to Code

Let me walk you through what happens when a user says "Create a calculator":

### 1. Session Initialization

```typescript
// User clicks "Start Session" ‚Üí handleConnect() fires
const handleConnect = async () => {
  // Generate unique channel name
  const channel = `agora-ai-${Math.random().toString(36).substring(2, 15)}`;

  // Get RTC token with both RTC and RTM2 privileges
  const response = await fetch("/api/token", {
    method: "POST",
    body: JSON.stringify({ channelName: channel, uid }),
  });

  // Start the AI agent
  const agentResponse = await fetch("/api/start-agent", {
    method: "POST",
    body: JSON.stringify({ channelName: channel, uid }),
  });

  // Initialize Agora client and join channel
  const client = new AgoraConversationalClient(/* ... */);
  await client.initialize();
};
```

**Why this matters**: We generate a random channel name for each session to ensure isolation. The token has both RTC (for audio) and RTM2 (for messages) privileges baked in, so we only need one token instead of managing two separately.

### 2. Token Generation (Server-Side)

The `/api/token` route generates a secure token that never exposes your App Certificate to the client:

```typescript
// app/api/token/route.ts
export async function POST(request: NextRequest) {
  const { channelName, uid } = await request.json();

  // Build token with BOTH RTC and RTM2 privileges
  const token = RtcTokenBuilder.buildTokenWithRtm2(
    appId,
    appCertificate,
    channelName,
    uid, // RTC account (numeric)
    RtcRole.PUBLISHER,
    3600, // 1 hour expiration
    3600,
    3600,
    3600,
    3600, // RTC privileges
    String(uid), // RTM user ID (string)
    3600 // RTM privilege
  );

  return NextResponse.json({ token });
}
```

**Security note**: Always generate tokens server-side. Your App Certificate should never touch the browser.

### 3. Starting the Conversational AI Agent

This is where the magic happens. The `/api/start-agent` route configures the entire AI pipeline:

```typescript
// app/api/start-agent/route.ts
const requestBody = {
  name: `agent-${channelName}-${Date.now()}`,
  properties: {
    channel: channelName,
    token: botToken,
    agent_rtc_uid: botUid,
    remote_rtc_uids: ["*"], // Listen to all users in channel

    // Enable smart features
    advanced_features: {
      enable_aivad: true, // AI Voice Activity Detection (interruption)
      enable_rtm: true, // Real-time messaging for transcripts
    },

    // ASR: Speech-to-text
    asr: {
      language: "en-US",
      vendor: "ares", // Agora's ASR engine
    },

    // TTS: Text-to-speech
    tts: {
      vendor: "microsoft",
      params: {
        voice_name: "en-US-AndrewMultilingualNeural",
      },
      skip_patterns: [2], // Skip content in „Äê„Äë brackets
    },

    // LLM: The brain
    llm: {
      url: "https://api.openai.com/v1/chat/completions",
      api_key: llmApiKey,
      system_messages: [
        {
          role: "system",
          content: "You are an expert web development AI assistant...",
        },
      ],
      params: {
        model: "gpt-4o",
      },
    },
  },
};
```

**The skip_patterns trick**: Notice `skip_patterns: [2]`? This tells the TTS engine to skip content wrapped in Chinese square brackets `„Äê„Äë`. That's how we prevent the AI from reading aloud 500 lines of HTML code.

### 4. The Critical System Prompt

Here's the system prompt that makes the code generation work:

````text
You are an expert web development AI assistant. Keep spoken responses SHORT and concise.

IMPORTANT: When you generate HTML/CSS/JS code, you MUST wrap it in CHINESE SQUARE BRACKETS like this:
„Äê<!DOCTYPE html><html>...</html>„Äë

The Chinese square brackets „Äê„Äë are REQUIRED - they tell the system to render the code visually instead of speaking it.

RULES:
1. Code must be wrapped in Chinese square brackets: „Äê<!DOCTYPE html><html>...</html>„Äë
2. Put ONLY the raw HTML code inside „Äê„Äë - NO markdown code fences like ```html
3. Start with <!DOCTYPE html> or <html immediately after the opening „Äê
4. Text outside „Äê„Äë will be spoken aloud - KEEP IT BRIEF
5. Make code self-contained with inline CSS in <style> tags and JS in <script> tags
6. Use modern, clean design with good UX practices
7. For images, use https://picsum.photos/

CORRECT EXAMPLE:
Here's a button „Äê<!DOCTYPE html><html>...</html>„Äë that shows an alert.

WRONG EXAMPLE:
„Äê```html
<!DOCTYPE html>...
```„Äë
````

**Why Chinese brackets?** Regular brackets `[]` conflict with JavaScript arrays and JSON. Markdown fences break the TTS skip pattern. Chinese brackets are unique, rarely appear in natural conversation, and work perfectly with `skip_patterns: [2]`.

### 5. Real-Time Audio & Messaging

Once the agent joins the channel, we establish two parallel connections:

#### RTC Connection (Audio)

```typescript
// lib/agora-client.ts
async initialize() {
  // Create RTC client for audio
  this.client = AgoraRTC.createClient({ mode: "rtc", codec: "vp8" });

  // Listen for bot's audio
  this.client.on("user-published", async (user, mediaType) => {
    await this.client.subscribe(user, mediaType);

    if (mediaType === "audio" && user.uid === this.botUid) {
      user.audioTrack?.play(); // Play AI's voice
    }
  });

  // Join channel
  await this.client.join(this.appId, this.channel, this.token, this.uid);

  // Start sending our voice
  this.localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack();
  await this.client.publish([this.localAudioTrack]);
}
```

#### RTM Connection (Messages)

```typescript
// lib/agora-client.ts
private async initializeRTM() {
  const { RTM } = AgoraRTM;

  // Create RTM client (uses same token)
  this.rtmClient = new RTM(this.appId, String(this.uid), {
    useStringUserId: true,
  });

  await this.rtmClient.login({ token: this.token });
  await this.rtmClient.subscribe(this.channel, {
    withMessage: true,
    withPresence: true,
  });

  // Listen for transcription messages
  this.rtmClient.addEventListener("message", (event) => {
    const data = JSON.parse(event.message);

    // Detect message type
    const isAgent = data.object === "assistant.transcription";
    const isFinal = data.turn_status === 1 || data.final === true;

    // Send to UI callback
    if (this.onTranscription) {
      this.onTranscription({
        type: isAgent ? "agent" : "user",
        text: data.text,
        isFinal: isFinal,
        timestamp: Date.now(),
      });
    }
  });
}
```

**Why two connections?** RTC handles the actual audio streaming (low-latency, high-quality voice). RTM sends structured data like transcriptions, which we need for displaying the conversation and detecting code blocks.

### 6. Parsing the AI's Response

When the AI responds, we need to:

1. Separate spoken text from code
2. Display spoken text in the transcript
3. Render code in the preview pane

````typescript
// app/page.tsx
const parseAgentResponse = (text: string) => {
  // Regex to find content between „Äê„Äë
  const codeRegex = /„Äê[\s\S]*?„Äë/gi;

  const codes: string[] = [];
  let spokenText = text;

  // Extract all code blocks
  const matches = Array.from(text.matchAll(codeRegex));
  for (const match of matches) {
    // Remove the „Äê„Äë brackets
    let content = match[0].slice(1, -1);

    // Clean up any markdown fences if AI added them
    content = content.replace(/^```[\w]*\n?/g, "").replace(/```$/g, "");
    content = content.trim();

    // Validate it's HTML
    if (content.includes("<html") || content.includes("<!DOCTYPE")) {
      codes.push(content);
      spokenText = spokenText.replace(match[0], ""); // Remove from spoken text
    }
  }

  return {
    spokenText: spokenText.trim(), // Text to display
    codes, // Code to render
  };
};
````

### 7. Smart Loading Indicators

Users need to know when the AI is generating code. We detect this by watching for the Chinese opening bracket:

```typescript
// Set up transcription callback
client.setTranscriptionCallback((message) => {
  const { spokenText, codes } = parseAgentResponse(message.text);

  // Detect code generation in progress
  const hasChineseOpenBracket = message.text?.includes("„Äê");

  if (message.type === "agent" && hasChineseOpenBracket) {
    if (!message.isFinal) {
      // AI is streaming code - show loading spinner
      setIsGeneratingCode(true);
    } else {
      // Code generation complete
      setIsGeneratingCode(false);
    }
  }

  // Display spoken text in transcript (only final messages)
  if (spokenText && message.isFinal) {
    setTranscript((prev) => [
      ...prev,
      {
        type: message.type,
        text: spokenText,
        timestamp: new Date(),
      },
    ]);
  }

  // Render code in preview (only final messages)
  if (codes.length > 0 && message.isFinal) {
    codes.forEach((code) => {
      setCodeBlocks((prev) => [...prev, { html: code, timestamp: new Date() }]);
      setCurrentCode(code);
    });
  }
});
```

**Why check for `isFinal`?** The AI streams responses word-by-word. We don't want to display partial sentences or render incomplete code. Only when `isFinal` is true do we know we have the complete message.

### 8. Safe Code Preview

Generated code runs in a sandboxed iframe to prevent XSS attacks:

```tsx
<iframe
  srcDoc={currentCode}
  title="Code Preview"
  sandbox="allow-scripts allow-forms allow-modals allow-popups allow-same-origin"
  style={{ display: "block", overflow: "auto" }}
/>
```

**Security layers**:

- `sandbox` attribute restricts what the code can do
- `allow-scripts` lets JS run (needed for interactivity)
- `allow-same-origin` enables localStorage but still isolates from parent page
- No `allow-top-navigation` means code can't redirect the main page

### 9. Graceful Disconnection

When the user clicks "End", we properly clean up resources:

```typescript
const handleDisconnect = async () => {
  // Stop the AI agent
  if (agentId) {
    await fetch("/api/leave-agent", {
      method: "POST",
      body: JSON.stringify({ agentId }),
    });
  }

  // Disconnect Agora client
  if (agoraClientRef.current) {
    await agoraClientRef.current.disconnect();
    agoraClientRef.current = null;
  }

  // Reset connection state but KEEP the preview and code
  setIsConnected(false);
  setIsMicActive(false);
  setTranscript([]);
  // Note: We DON'T clear codeBlocks or currentCode here
};
```

**New behavior**: The preview and code remain visible after ending the session. This lets users examine the results without the session running. Only when starting a new session do we reset everything.

### 10. Version Control

The app tracks all code iterations, so users can roll back:

```tsx
{
  codeBlocks.length > 1 && (
    <select
      value={codeBlocks.findIndex((b) => b.html === currentCode)}
      onChange={(e) => {
        const idx = parseInt(e.target.value);
        setCurrentCode(codeBlocks[idx].html);
      }}
    >
      {codeBlocks.map((block, idx) => (
        <option key={block.id} value={idx}>
          v{idx + 1} - {new Date(block.timestamp).toLocaleTimeString()}
        </option>
      ))}
    </select>
  );
}
```

This dropdown appears when the AI has generated multiple versions, letting users compare iterations.

## Key Implementation Details

### Token Management

Both the user and the bot need tokens, but they're generated differently:

**User Token** (`/api/token`):

- Generated per session with random UID
- Has both RTC + RTM2 privileges
- Used by browser to join channel

**Bot Token** (`/api/start-agent`):

- Generated with fixed `NEXT_PUBLIC_AGORA_BOT_UID`
- Also has RTC + RTM2 privileges
- Passed to Agora's Conversational AI service

Both use `buildTokenWithRtm2()` because we need RTM for transcripts.

### Managing Audio State

The microphone has multiple states:

- **Not started**: No audio track exists
- **Active**: Publishing audio to channel
- **Muted**: Audio track exists but disabled

```typescript
// Start microphone
async startMicrophone() {
  this.localAudioTrack = await AgoraRTC.createMicrophoneAudioTrack({
    encoderConfig: "speech_standard", // Optimize for voice
  });
  await this.client.publish([this.localAudioTrack]);
}

// Mute (keeps track alive)
async setMuted(muted: boolean) {
  if (this.localAudioTrack) {
    await this.localAudioTrack.setEnabled(!muted);
  }
}

// Stop completely
async stopMicrophone() {
  if (this.localAudioTrack) {
    this.localAudioTrack.stop();
    this.localAudioTrack.close();
    this.localAudioTrack = null;
  }
}
```

**Why this separation?** Muting is fast and reversible (UI toggle). Stopping destroys the track and requires re-initializing the microphone (permission prompt).

### Handling Interruptions

The `enable_aivad: true` setting enables AI Voice Activity Detection:

```typescript
advanced_features: {
  enable_aivad: true,  // Let user interrupt the AI
},
vad: {
  mode: "interrupt",
  interrupt_duration_ms: 160,     // How long user speaks to interrupt
  silence_duration_ms: 640,       // Silence before AI responds
},
```

This creates natural back-and-forth conversations. If the AI starts talking but you interrupt with "wait, stop", it actually stops and listens.

### Code Formatting

Raw AI-generated HTML is often minified. We format it for readability:

```typescript
function formatHTML(html: string): string {
  let formatted = "";
  let indent = 0;
  const tab = "  ";

  html.split(/(<[^>]+>)/g).forEach((token) => {
    if (!token.trim()) return;

    if (token.startsWith("</")) {
      // Closing tag - decrease indent
      indent = Math.max(0, indent - 1);
      formatted += "\n" + tab.repeat(indent) + token;
    } else if (token.startsWith("<") && !token.endsWith("/>")) {
      // Opening tag - add then increase indent
      formatted += "\n" + tab.repeat(indent) + token;
      if (!token.match(/<(br|hr|img|input|meta|link)/i)) {
        indent++;
      }
    } else {
      // Text content
      const trimmed = token.trim();
      if (trimmed) {
        formatted += "\n" + tab.repeat(indent) + trimmed;
      }
    }
  });

  return formatted.trim();
}
```

This is used in the "Source Code" view to make the HTML readable.

## Common Pitfalls & Solutions

### Issue 1: AI Reads Code Aloud

**Problem**: Without `skip_patterns`, the AI will attempt to speak every character of HTML code. It sounds like gibberish and takes forever.

**Solution**:

```typescript
tts: {
  skip_patterns: [2],  // Pattern 2 = Chinese square brackets „Äê„Äë
}
```

And ensure your system prompt explicitly tells the AI to use these brackets.

### Issue 2: RTM Connection Fails

**Problem**: "RTM login failed" or "Invalid token"

**Solutions**:

- Verify your token has RTM2 privileges (use `buildTokenWithRtm2`, not `buildTokenWithUid`)
- UID must be a string for RTM but numeric for RTC - pass both formats
- Ensure your Agora project has RTM enabled

### Issue 3: Code Not Rendering

**Problem**: AI generates code but nothing appears in preview

**Checklist**:

- Check browser console for `parseAgentResponse` logs
- Verify the AI is using „Äê„Äë brackets (check transcript)
- Look for `<!DOCTYPE html>` or `<html` in the code
- Ensure `isFinal` is true before rendering

### Issue 4: Bot Not Speaking

**Problem**: Can see transcript but hear no audio

**Solutions**:

- Verify `NEXT_PUBLIC_AGORA_BOT_UID` matches your start-agent config
- Check that bot UID is subscribed in RTC `user-published` event
- Ensure browser audio isn't muted
- Look for "Bot disconnected" logs

### Issue 5: Microphone Won't Start

**Problem**: "Permission denied" or "No microphone found"

**Solutions**:

- Check browser permissions (should prompt automatically)
- Ensure another app isn't using the microphone
- Try in a different browser (Chrome/Edge recommended)
- Use HTTPS in production (required for `getUserMedia`)

## Deployment Considerations

### Environment Variables

Never commit these to git:

```bash
# .env.local (DO NOT COMMIT)
NEXT_PUBLIC_AGORA_APP_ID=abc123...
AGORA_APP_CERTIFICATE=xyz789...
AGORA_CUSTOMER_ID=cust123...
AGORA_CUSTOMER_SECRET=secret456...
NEXT_PUBLIC_AGORA_BOT_UID=1001
LLM_URL=https://api.openai.com/v1/chat/completions
LLM_API_KEY=sk-...
TTS_API_KEY=...
TTS_REGION=eastus
```

**For production**:

- Set these in your hosting platform (Vercel, Netlify, AWS, etc.)
- Use separate Agora projects for dev/staging/prod
- Rotate secrets regularly

### Build & Deploy

```bash
# Install dependencies
npm install

# Build production bundle
npm run build

# Start production server
npm start
```

The app is fully server-side rendered with Next.js. Static pages are pre-rendered, API routes run on-demand.

### HTTPS Requirement

Browsers require HTTPS for:

- `getUserMedia` (microphone access)
- Secure WebSocket connections
- Service Workers

In development, `localhost` is treated as secure. In production, use a valid SSL certificate.

### Cost Optimization

Agora Conversational AI charges for:

1. **Audio duration**: Per minute of voice interaction
2. **LLM usage**: GPT-4o tokens (input + output)
3. **TTS**: Characters spoken

**Tips to reduce costs**:

- Set `idle_timeout: 120` to auto-disconnect inactive sessions
- Keep system prompts concise
- Use shorter greeting messages
- Consider GPT-3.5-turbo for simple requests
- Cache common responses if possible

## Testing Locally

### Quick Start

```bash
# 1. Clone the repo
git clone <your-repo>
cd la_tech_week

# 2. Install dependencies
npm install

# 3. Create .env.local with your credentials
cp .env.example .env.local
# Edit .env.local with your actual keys

# 4. Start dev server
npm run dev

# 5. Open http://localhost:3000
```

### Test Scenarios

**Basic Connection**:

1. Click "Start Session"
2. Allow microphone access
3. Wait for "Microphone active" message
4. Say "Hello" - AI should respond

**Code Generation**:

1. Say "Create a button that says hello"
2. Watch for "Generating code..." spinner
3. Code should appear in preview pane
4. Try clicking the button

**Version Control**:

1. Generate initial code
2. Say "Make it blue instead"
3. Version dropdown should show v1 and v2
4. Switch between versions

**Session Persistence**:

1. Generate some code
2. Click "End" button
3. Preview should still show the code
4. Click "Start Session" again
5. Preview should reset

### Debugging Tips

**Enable verbose logging**:

```typescript
// lib/agora-client.ts
AgoraRTC.setLogLevel(0); // 0 = debug, 4 = none
```

**Monitor network traffic**:

- Open browser DevTools ‚Üí Network tab
- Filter by "WS" to see WebSocket connections
- Check `/api/token` and `/api/start-agent` responses

**Check RTM messages**:
All RTM messages are logged to console with:

```
üì® RTM MESSAGE: { ... }
```

Look for `object: "assistant.transcription"` for AI responses.

## Extending the App

### Add Code Export

Want to download as a complete project instead of just HTML?

```typescript
import JSZip from "jszip";

async function exportProject(html: string) {
  const zip = new JSZip();

  // Extract inline CSS
  const cssMatch = html.match(/<style>([\s\S]*?)<\/style>/);
  const css = cssMatch ? cssMatch[1] : "";

  // Extract inline JS
  const jsMatch = html.match(/<script>([\s\S]*?)<\/script>/);
  const js = jsMatch ? jsMatch[1] : "";

  // Create separate files
  zip.file("index.html", html);
  if (css) zip.file("styles.css", css);
  if (js) zip.file("script.js", js);
  zip.file("README.md", "# Generated by AI Coding Assistant");

  // Download
  const blob = await zip.generateAsync({ type: "blob" });
  const url = URL.createObjectURL(blob);
  const a = document.createElement("a");
  a.href = url;
  a.download = `project-${Date.now()}.zip`;
  a.click();
}
```

### Add Syntax Highlighting

Install Prism.js for beautiful code display:

```bash
npm install prismjs @types/prismjs
```

```typescript
import Prism from "prismjs";
import "prismjs/themes/prism-tomorrow.css";

function SourceCodeView({ code }: { code: string }) {
  const highlighted = Prism.highlight(
    formatHTML(code),
    Prism.languages.html,
    "html"
  );

  return (
    <pre className="language-html">
      <code dangerouslySetInnerHTML={{ __html: highlighted }} />
    </pre>
  );
}
```

### Add Voice Commands

Implement special commands that don't generate code:

```typescript
const handleVoiceCommand = (text: string) => {
  const lower = text.toLowerCase();

  if (lower.includes("clear preview")) {
    setCurrentCode("");
    return true;
  }

  if (lower.includes("show version")) {
    // Show version picker
    return true;
  }

  if (lower.includes("export code")) {
    exportProject(currentCode);
    return true;
  }

  return false; // Not a command, let AI handle it
};
```

Update the transcription callback:

```typescript
client.setTranscriptionCallback((message) => {
  if (message.type === "user" && message.isFinal) {
    const wasCommand = handleVoiceCommand(message.text);
    if (wasCommand) return; // Don't send to AI
  }

  // Normal processing...
});
```

### Add Multi-Language Support

The ASR and TTS support multiple languages:

```typescript
// In start-agent route
asr: {
  language: userSelectedLanguage, // "en-US", "es-ES", "zh-CN", etc.
  vendor: "ares",
},
tts: {
  vendor: "microsoft",
  params: {
    voice_name: getVoiceForLanguage(userSelectedLanguage),
  },
},
```

Voice options:

- English: `en-US-AndrewMultilingualNeural`
- Spanish: `es-ES-AlvaroNeural`
- French: `fr-FR-HenriNeural`
- Chinese: `zh-CN-YunxiNeural`

See [Azure TTS voice list](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=tts) for all options.

## Performance Optimizations

### Reduce Initial Load Time

The Agora SDKs are large. Use dynamic imports:

```typescript
// Instead of:
import AgoraRTC from "agora-rtc-sdk-ng";

// Use:
const AgoraRTC = await import("agora-rtc-sdk-ng");
```

This is already done in `handleConnect`:

```typescript
const AgoraModule = await import("@/lib/agora-client");
const client = new AgoraModule.AgoraConversationalClient(/* ... */);
```

### Optimize Re-renders

Code blocks can be large. Memoize the preview:

```typescript
import { memo } from "react";

const CodePreview = memo(({ code }: { code: string }) => {
  return (
    <iframe
      srcDoc={code}
      title="Code Preview"
      sandbox="allow-scripts allow-forms allow-modals"
    />
  );
});
```

### Debounce State Updates

During code streaming, the AI might send many interim messages:

```typescript
import { useMemo } from "react";
import debounce from "lodash.debounce";

const debouncedSetIsGenerating = useMemo(
  () => debounce(setIsGeneratingCode, 300),
  []
);
```

## Real-World Use Cases

Beyond just demos, this architecture enables:

### 1. Interactive Coding Tutorials

Students can ask questions while learning:

- "Show me how to center a div"
- "What's the difference between flexbox and grid?"
- "Create an example of async/await"

Each answer comes with working code they can immediately test.

### 2. Rapid Prototyping

Product managers can describe features in plain English:

- "Make a pricing table with three tiers"
- "Add a contact form with validation"
- "Show me what the mobile view would look like"

No Figma required - see the actual UI in seconds.

### 3. Accessibility Testing

Generate test cases with built-in accessibility:

- "Create a form with proper ARIA labels"
- "Show me a keyboard-navigable menu"
- "Build a screen-reader-friendly modal"

The AI follows best practices automatically.

### 4. Client Presentations

Show clients real, interactive mockups during calls:

- "Let me show you what this would look like..."
- _Speaks to AI, generates UI live_
- Client can actually click and interact

Way more impressive than static slides.

## What's Next?

This is just the beginning. Here's what we're considering for v2:

- **Multi-file projects**: Generate React components, not just single HTML files
- **Live collaboration**: Multiple users voice-coding together
- **Code history**: Git-like versioning with diffs
- **Deploy button**: One-click deployment to Vercel/Netlify
- **Custom components**: Train the AI on your design system
- **Visual editing**: Point and say "make that button bigger"

## Conclusion

Building this voice-powered coding assistant taught me that the future of development tools isn't just about writing code faster - it's about removing the barrier between thinking and building.

When you can say "create a todo list" and see a working app 10 seconds later, you're not just saving time. You're freeing your mind to focus on the creative parts: the UX, the interactions, the problem you're actually solving.

The Agora Conversational AI platform handles the heavy lifting:

- Crystal-clear voice transmission via RTC
- Real-time transcription via RTM
- Seamless LLM integration
- Natural-sounding TTS

All we had to do was wire it together and build a great UI.

If you build something with this architecture, I'd love to see it. Tag [@AgoraIO](https://twitter.com/agoraio) and show us what you create.

Now stop reading and start building. üöÄ

---

## Quick Reference

### Key Packages

```json
{
  "agora-rtc-sdk-ng": "^4.20.0", // Audio streaming
  "agora-rtm-sdk": "^2.2.2", // Real-time messaging
  "agora-token": "^2.0.5", // Token generation
  "next": "^14.0.0", // Framework
  "jszip": "^3.10.1" // Code export
}
```

### Essential API Endpoints

**Start Agent**:

```
POST https://api.agora.io/api/conversational-ai-agent/v2/projects/{appId}/join
```

**Leave Agent**:

```
POST https://api.agora.io/api/conversational-ai-agent/v2/projects/{appId}/agents/{agentId}/leave
```

### Token Generation

```typescript
import { RtcTokenBuilder, RtcRole } from "agora-token";

const token = RtcTokenBuilder.buildTokenWithRtm2(
  appId, // Your Agora App ID
  appCertificate, // Your App Certificate
  channelName, // Channel name
  uid, // Numeric UID for RTC
  RtcRole.PUBLISHER, // Role
  3600, // RTC token expiration
  3600,
  3600,
  3600,
  3600, // RTC privileges
  String(uid), // String UID for RTM
  3600 // RTM token expiration
);
```

### Useful Resources

- [Agora Conversational AI Docs](https://docs.agora.io/en/conversational-ai/overview)
- [Agora RTC SDK Reference](https://api-ref.agora.io/en/voice-sdk/web/4.x/index.html)
- [Agora RTM SDK Reference](https://api-ref.agora.io/en/signaling/web/2.x/index.html)
- [Azure TTS Voice Gallery](https://speech.microsoft.com/portal/voicegallery)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)

### Environment Variables Template

```bash
# Agora Credentials
NEXT_PUBLIC_AGORA_APP_ID=
AGORA_APP_CERTIFICATE=
AGORA_CUSTOMER_ID=
AGORA_CUSTOMER_SECRET=
NEXT_PUBLIC_AGORA_BOT_UID=1001

# LLM Configuration
LLM_URL=https://api.openai.com/v1/chat/completions
LLM_API_KEY=

# TTS Configuration
TTS_API_KEY=
TTS_REGION=eastus
```

### Contact & Support

- **Agora Developer Support**: support@agora.io
- **Agora Console**: https://console.agora.io
- **Community Slack**: https://www.agora.io/en/community/

Built with ‚ù§Ô∏è for LA Tech Week by the Agora team.
